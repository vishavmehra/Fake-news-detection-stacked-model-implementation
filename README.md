# üì∞ Fake News Detection using Deep Learning, Machine Learning & Custom Stacking Ensemble

This project compares **Deep Learning** (Uni-LSTM, Bi-LSTM, Uni-GRU, Bi-GRU, CNN) and **Machine Learning** (SVM, Logistic Regression, Decision Tree, KNN, Random Forest) models for **Fake News Detection** using the **ISOT Fake News Dataset**.

A final **stacked ensemble model** is constructed using predictions from all models. This ensemble achieves **up to 99.85% accuracy**, outperforming individual base models.

---

## üìÅ Dataset: ISOT Fake News Dataset

| File       | Description         |
|------------|---------------------|
| `True.csv` | Real news articles  |
| `Fake.csv` | Fake news articles  |

A binary `label` column is added:
- `0` ‚Üí Real
- `1` ‚Üí Fake

---

## üßπ Preprocessing Pipeline

Text preprocessing steps include:
- Lowercasing
- Punctuation, number, and special character removal
- Stopword removal using **NLTK**
- Character normalization (e.g., "sooo" ‚Üí "so")
- Lemmatization using **spaCy** (`en_core_web_sm`)
- Final whitespace cleanup

---

## üî† Tokenization & Vectorization

| Model Type | Vectorizer          | Description                                 |
|------------|---------------------|---------------------------------------------|
| DL Models  | Keras Tokenizer     | Tokenization + padding for sequence models  |
| ML Models  | TF-IDF              | For all ML models except RF2                |
| RF2 Model  | CountVectorizer     | Used only for Random Forest 2 (TF input)    |

---

## üß† Deep Learning Architectures

| Model     | Layers |
|-----------|--------|
| Uni-LSTM  | Embedding ‚Üí LSTM ‚Üí Dense + Dropout ‚Üí Sigmoid |
| Bi-LSTM   | Embedding ‚Üí Bidirectional LSTM ‚Üí Dense ‚Üí Sigmoid |
| Uni-GRU   | Embedding ‚Üí GRU ‚Üí Dense + Dropout ‚Üí Sigmoid |
| Bi-GRU    | Embedding ‚Üí Bidirectional GRU ‚Üí Dense ‚Üí Sigmoid |
| CNN       | Embedding ‚Üí Conv1D ‚Üí GlobalMaxPooling ‚Üí Dense ‚Üí Sigmoid |

**Common DL Settings:**
- `embedding_dim = 128`
- `max_len = 300`
- `optimizer = Adam`, `loss = BinaryCrossentropy`
- `EarlyStopping` used to avoid overfitting

---

## üß† Machine Learning Models (Scikit-learn)

| Model                          | Parameters                         |
|--------------------------------|------------------------------------|
| SVM                            | `C=1`                              |
| Logistic Regression            | `C=1`, `max_iter=1000`             |
| Decision Tree                  | `max_depth=5`                      |
| K-Nearest Neighbors            | `k=9`                              |
| Random Forest 1 (TF-IDF)       | `n_estimators=400`, `max_depth=40` |
| Random Forest 2 (TF only)      | `n_estimators=300`, `max_depth=40` |

---

## üß© Stacked Ensemble Classifier

A **Random Forest-based meta-classifier** was trained on the predictions from:
- 5 Deep Learning models (Uni-LSTM, Bi-LSTM, Uni-GRU, Bi-GRU, CNN)
- 6 Machine Learning models (SVM, LR, DT, KNN, RF1, RF2)

### üß† Meta-model:
```python
RandomForestClassifier(n_estimators=300, max_depth=40, random_state=42)
